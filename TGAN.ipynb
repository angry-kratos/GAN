{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Library to Display gifs:\n",
      "Collecting moviepy\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n",
      "     -------------------------------------- 388.3/388.3 KB 2.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from moviepy) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from moviepy) (2.26.0)\n",
      "Collecting proglog<=1.0.0\n",
      "  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from moviepy) (1.25.1)\n",
      "Collecting imageio<3.0,>=2.5\n",
      "  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n",
      "     -------------------------------------- 313.2/313.2 KB 6.4 MB/s eta 0:00:00\n",
      "Collecting imageio_ffmpeg>=0.2.0\n",
      "  Downloading imageio_ffmpeg-0.4.8-py3-none-win_amd64.whl (22.6 MB)\n",
      "     ---------------------------------------- 22.6/22.6 MB 9.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (9.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\akarsh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.4)\n",
      "Building wheels for collected packages: moviepy\n",
      "  Building wheel for moviepy (setup.py): started\n",
      "  Building wheel for moviepy (setup.py): finished with status 'done'\n",
      "  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110743 sha256=2f2bf72199b509a9bfa1db2d7795ce0f695e32f85d778ccdbfb58a13966b9abb\n",
      "  Stored in directory: c:\\users\\akarsh\\appdata\\local\\pip\\cache\\wheels\\96\\32\\2d\\e10123bd88fbfc02fed53cc18c80a171d3c87479ed845fa7c1\n",
      "Successfully built moviepy\n",
      "Installing collected packages: imageio_ffmpeg, imageio, decorator, proglog, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed decorator-4.4.2 imageio-2.31.1 imageio_ffmpeg-0.4.8 moviepy-1.0.3 proglog-0.1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Akarsh\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pre-trained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!echo Installing Library to Display gifs:\n",
    "!pip install moviepy\n",
    "!echo Downloading pre-trained weights\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mk9JdmJH79_vtQkl8zk-jDxa7xUXpck-' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1mk9JdmJH79_vtQkl8zk-jDxa7xUXpck-\" -O state_normal81000.ckpt && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import Image\n",
    "\n",
    "def genSamples(g, n=8):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s = g(torch.rand((n**2, 100), device='cuda')*2-1).cpu().detach().numpy()\n",
    "    \n",
    "    out = np.zeros((3, 16, 64*n, 64*n))\n",
    "    \n",
    "    for j in range(n):\n",
    "        for k in range(n):\n",
    "            out[:, :, 64*j:64*(j+1), 64*k:64*(k+1)] = s[j*n+k, :, :, :, :]\n",
    "\n",
    "    \n",
    "    out = out.transpose((1, 2, 3, 0))\n",
    "    out = (out + 1) / 2 * 255\n",
    "    out = out.astype(int)\n",
    "    clip = ImageSequenceClip(list(out), fps=20)\n",
    "    clip.write_gif('sample.gif', fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a sequential model to turn one vector into 16\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose1d(100, 512, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 100, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # initialize weights according to paper\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if type(m) == nn.ConvTranspose1d:\n",
    "            nn.init.xavier_uniform_(m.weight, gain=2**0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshape x so that it can have convolutions done \n",
    "        x = x.view(-1, 100, 1)\n",
    "        # apply the model and flip the \n",
    "        x = self.model(x).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # instantiate the temporal generator\n",
    "        self.temp = TemporalGenerator()\n",
    "\n",
    "        # create a transformation for the temporal vectors\n",
    "        self.fast = nn.Sequential(\n",
    "            nn.Linear(100, 256 * 4**2, bias=False),\n",
    "            nn.BatchNorm1d(256 * 4**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # create a transformation for the content vector\n",
    "        self.slow = nn.Sequential(\n",
    "            nn.Linear(100, 256 * 4**2, bias=False),\n",
    "            nn.BatchNorm1d(256 * 4**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "        # define the image generator\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # initialize weights according to the paper\n",
    "        self.fast.apply(self.init_weights)\n",
    "        self.slow.apply(self.init_weights)\n",
    "        self.model.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if type(m) == nn.ConvTranspose2d or type(m) == nn.Linear:\n",
    "            nn.init.uniform_(m.weight, a=-0.01, b=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass our latent vector through the temporal generator and reshape\n",
    "        z_fast = self.temp(x).contiguous()\n",
    "        z_fast = z_fast.view(-1, 100)\n",
    "\n",
    "        # transform the content and temporal vectors \n",
    "        z_fast = self.fast(z_fast).view(-1, 256, 4, 4)\n",
    "        z_slow = self.slow(x).view(-1, 256, 4, 4).unsqueeze(1)\n",
    "        # after z_slow is transformed and expanded we can duplicate it\n",
    "        z_slow = torch.cat([z_slow]*16, dim=1).view(-1, 256, 4, 4)\n",
    "\n",
    "        # concatenate the temporal and content vectors\n",
    "        z = torch.cat([z_slow, z_fast], dim=1)\n",
    "\n",
    "        # transform into image frames\n",
    "        out = self.model(z)\n",
    "\n",
    "        return out.view(-1, 16, 3, 64, 64).transpose(1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
